# Sprawozdanie z Laboratorium 10: Azure Kubernetes Service (AKS) ğŸš€

## Cel Laboratorium ğŸ¯
Celem laboratorium byÅ‚o zapoznanie siÄ™ z platformÄ… Kubernetes (K8s) do wdraÅ¼ania aplikacji z Docker Registry na klastrze Azure Kubernetes Service (AKS). Wykonano zadania zwiÄ…zane z aktualizacjÄ… repozytorium, budowÄ… i wypchniÄ™ciem obrazu Docker do Azure Container Registry (ACR), konfiguracjÄ… poÅ‚Ä…czenia z klastrem AKS, wdroÅ¼eniem aplikacji na klaster oraz przeprowadzeniem testÃ³w funkcjonalnych. Przygotowano sprawozdanie w formacie Markdown dokumentujÄ…ce wszystkie kroki.

---

## 1. Aktualizacja Repozytorium ğŸ“‚

### 1.1 Zaktualizowanie metadanych projektu
Pobrano wszystkie metadane projektu z repozytorium za pomocÄ… polecenia:
```bash
git fetch --all
```

### 1.2 PrzeÅ‚Ä…czenie na branch `main`
PrzeÅ‚Ä…czono siÄ™ na gÅ‚Ã³wny branch repozytorium:
```bash
git checkout main
```

### 1.3 Pobranie zmian w kodzie
Zaktualizowano lokalnÄ… kopiÄ™ brancha `main`:
```bash
git pull
```

### 1.4 Stworzenie brancha roboczego
Utworzono nowy branch roboczy o nazwie `Lab12/417298` i przeÅ‚Ä…czono siÄ™ na niego:
```bash
git checkout -b Lab12/417298
```

### 1.5 Stworzenie folderu roboczego
Utworzono folder `env_417298` do pracy nad zadaniem:
```bash
mkdir env_417298
cd env_417298
```

![Kroki aktualizacji repozytorium i tworzenie folderu](ZdjÄ™cia/1.png)

---

## 2. Budowa i WypchniÄ™cie Obrazu Docker ğŸ³

### 2.1 Konfiguracja infrastruktury z Terraform
W folderze `terraform` przygotowano infrastrukturÄ™ dla Azure Container Registry (ACR) i klastra AKS. PrzykÅ‚adowy plik `main.tf`:
```hcl
provider "azurerm" {
  features {}
}

resource "azurerm_resource_group" "rg" {
  name     = "rg-aks-417298"
  location = "West Europe"
}

resource "azurerm_container_registry" "acr" {
  name                = "acr417298"
  resource_group_name = azurerm_resource_group.rg.name
  location            = azurerm_resource_group.rg.location
  sku                 = "Basic"
  admin_enabled       = true
}

resource "azurerm_kubernetes_cluster" "aks" {
  name                = "aks-417298"
  location            = azurerm_resource_group.rg.location
  resource_group_name = azurerm_resource_group.rg.name
  dns_prefix          = "aks417298"

  default_node_pool {
    name       = "default"
    node_count = 1
    vm_size    = "Standard_D2_v2"
  }

  identity {
    type = "SystemAssigned"
  }
}
```

WdroÅ¼ono infrastrukturÄ™:
```bash
terraform init
terraform plan
terraform apply -auto-approve
```

![WdroÅ¼enie infrastruktury Terraform](ZdjÄ™cia/2.png)

### 2.2 Budowa obrazu Docker
W folderze gÅ‚Ã³wnym repozytorium znajdowaÅ‚ siÄ™ plik `Dockerfile`. Dla procesorÃ³w ARM (np. Apple M1) uÅ¼yto narzÄ™dzia `buildx`:
```bash
docker buildx create --use
docker buildx build --platform linux/amd64 -t acr417298.azurecr.io/myapp:latest .
```

Dla procesorÃ³w x86 wykonano standardowÄ… komendÄ™:
```bash
docker build -t acr417298.azurecr.io/myapp:latest .
```

### 2.3 WypchniÄ™cie obrazu do ACR
Zalogowano siÄ™ do Azure Container Registry:
```bash
az acr login --name acr417298
```

WypchniÄ™to obraz do ACR:
```bash
docker push acr417298.azurecr.io/myapp:latest
```

![Budowa i wypchniÄ™cie obrazu Docker](ZdjÄ™cia/3.png)

---

## 3. PoÅ‚Ä…czenie z Klastrem AKS ğŸ”—

### 3.1 Logowanie do Azure CLI
Zalogowano siÄ™ do Azure CLI:
```bash
az login
```

### 3.2 PoÅ‚Ä…czenie z klastrem AKS
Pobrano poÅ›wiadczenia klastra AKS:
```bash
az aks get-credentials --resource-group rg-aks-417298 --name aks-417298
```

### 3.3 Testowanie kubectl
Sprawdzono poprawnoÅ›Ä‡ poÅ‚Ä…czenia z klastrem:
```bash
kubectl get nodes
```

![Test poÅ‚Ä…czenia z klastrem AKS](ZdjÄ™cia/4.png)

---

## 4. WdroÅ¼enie Obrazu na Klaster K8s i Testy Funkcjonalne âš™ï¸

### 4.1 Dodanie poÅ›wiadczeÅ„ ACR do klastra
Utworzono secret w Kubernetes dla ACR:
```bash
kubectl create secret docker-registry acr-secret \
  --docker-server=acr417298.azurecr.io \
  --docker-username=<ACR_ADMIN_USER> \
  --docker-password=<ACR_ADMIN_PASSWORD>
```

### 4.2 WdroÅ¼enie poda za pomocÄ… manifestu
Utworzono plik `deployment.yml`:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: acr417298.azurecr.io/myapp:latest
        ports:
        - containerPort: 8080
      imagePullSecrets:
      - name: acr-secret
```

WdroÅ¼ono deployment:
```bash
kubectl apply -f deployment.yml
```

### 4.3 Utworzenie serwisu
Utworzono plik `service.yml` dla serwisu typu ClusterIP:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  selector:
    app: myapp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
  type: ClusterIP
```

WdroÅ¼ono serwis:
```bash
kubectl apply -f service.yml
```

### 4.4 Port forwarding i testy funkcjonalne
Przekierowano port na localhost:
```bash
kubectl port-forward service/myapp-service 8080:80
```

Uruchomiono testy funkcjonalne z pliku `app_test.py`:
```bash
pytest app_test.py
```

![Port forwarding i wyniki testÃ³w](ZdjÄ™cia/5.png)

---

## 5. Sprawozdanie ğŸ“

### 5.1 Dokumentacja pracy
Sprawozdanie zostaÅ‚o przygotowane w formacie Markdown w folderze `env_417298/docs`. Zawiera szczegÃ³Å‚owy opis krokÃ³w, zrzuty ekranu oraz fragmenty kodu uÅ¼ytego w zadaniu.

### 5.2 Commit i push
Dodano pliki do brancha `Lab12/417298`:
```bash
git add .
git commit -m "Add AKS deployment and report for Lab12"
git push origin Lab12/417298
```

Utworzono Pull Request do brancha grupowego.

![Pull Request](ZdjÄ™cia/6.png)

---

## 6. Tematy Dodatkowe â“

### 6.1 RÃ³Å¼nice miÄ™dzy silnikiem containerd a Docker
- **Docker**: Platforma do konteneryzacji, ktÃ³ra obejmuje silnik kontenerÃ³w (dockerd), CLI oraz funkcje takie jak budowanie obrazÃ³w i zarzÄ…dzanie rejestrami. UÅ¼ywa `containerd` jako runtime, ale dodaje warstwy abstrakcji, co czyni go bardziej zasoboÅ¼ernym. Stosowany w Å›rodowiskach deweloperskich i CI/CD.
- **containerd**: Lekki runtime kontenerÃ³w, skoncentrowany na zarzÄ…dzaniu cyklem Å¼ycia kontenerÃ³w (tworzenie, uruchamianie, usuwanie). Jest bardziej minimalistyczny, co czyni go popularnym w klastrach K8s (np. w AKS). Nie obejmuje narzÄ™dzi do budowania obrazÃ³w ani zarzÄ…dzania rejestrami, co wymaga dodatkowych narzÄ™dzi (np. `nerdctl`).

**Zastosowanie**:
- Docker: Åšrodowiska deweloperskie, lokalne testy, CI/CD.
- containerd: Produkcyjne klastry K8s, gdzie kluczowa jest wydajnoÅ›Ä‡ i minimalizacja zasobÃ³w.

### 6.2 Inne sposoby zapewnienia poÅ‚Ä…czenia z aplikacjÄ…
OprÃ³cz `ClusterIP`, Kubernetes oferuje inne typy serwisÃ³w:
1. **NodePort**: UdostÄ™pnia aplikacjÄ™ na okreÅ›lonym porcie kaÅ¼dego wÄ™zÅ‚a klastra (zazwyczaj w zakresie 30000â€“32767). UÅ¼ywany do bezpoÅ›redniego dostÄ™pu do aplikacji z zewnÄ…trz klastra, ale wymaga znajomoÅ›ci adresu IP wÄ™zÅ‚a.
2. **LoadBalancer**: Tworzy zewnÄ™trzny balancer obciÄ…Å¼enia (np. w Azure), przypisujÄ…c publiczny adres IP do serwisu. Idealny dla aplikacji produkcyjnych wymagajÄ…cych dostÄ™pu z internetu.
3. **ExternalName**: Mapuje serwis na zewnÄ™trzny adres DNS bez tworzenia proxy w klastrze. UÅ¼ywany do integracji z usÅ‚ugami zewnÄ™trznymi.
4. **Ingress**: Zaawansowany mechanizm routingu HTTP/HTTPS, umoÅ¼liwiajÄ…cy dostÄ™p do wielu aplikacji pod jednym adresem IP za pomocÄ… reguÅ‚ opartych na Å›cieÅ¼kach lub domenach. Wymaga kontrolera Ingress (np. NGINX).

**RÃ³Å¼nice**:
- **ClusterIP**: DomyÅ›lny, wewnÄ™trzny dostÄ™p w klastrze, bez zewnÄ™trznego IP.
- **NodePort**: Otwiera port na wÄ™zÅ‚ach, mniej skalowalny, wymaga rÄ™cznego zarzÄ…dzania portami.
- **LoadBalancer**: Automatyczne przypisanie publicznego IP, kosztowne w chmurze.
- **Ingress**: Elastyczny routing HTTP, wymaga dodatkowej konfiguracji kontrolera.

---

## Podsumowanie ğŸ“
Laboratorium pozwoliÅ‚o na praktyczne zapoznanie siÄ™ z platformÄ… Kubernetes w Å›rodowisku Azure. Skonfigurowano infrastrukturÄ™ AKS i ACR za pomocÄ… Terraform, zbudowano i wypchniÄ™to obraz Docker, wdroÅ¼ono aplikacjÄ™ na klaster oraz przeprowadzono testy funkcjonalne. Sprawozdanie i kod zostaÅ‚y przesÅ‚ane do repozytorium zgodnie z wymaganiami, a tematy dodatkowe pogÅ‚Ä™biÅ‚y zrozumienie Kubernetes i konteneryzacji.